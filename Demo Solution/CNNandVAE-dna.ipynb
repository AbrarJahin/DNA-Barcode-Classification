{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9qh3WlDJlO5l"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "x_train = pd.read_csv('train_features.csv')\n",
    "x_test = pd.read_csv('test_features.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "y_test = pd.read_csv('test_labels.csv')\n",
    "y_train=y_train.labels\n",
    "y_test=y_test.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PT5ARMBmlO5n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        AACATTATACTTTATTTTCGGAGCATGATCAGGAATAGTAGGAACT...\n",
      "1        TACACTATACTTCATTTTTGGTGCTTGAGCAGGAATGCTAGGAACA...\n",
      "2        ----------------------------------------------...\n",
      "3        AACATTATATTTTATTTTTGGTGCATGAGCTGGAATAGTAGGAACT...\n",
      "4        AACTTTATATTTTATTTTTGGAGCTTGAGCTGGAATAGTTGGAACA...\n",
      "                               ...                        \n",
      "12901    TACTCTGTATTTTCTATTTGGAGTATGATCAGGAATAGTAGGAACA...\n",
      "12902    AACATTATATTTTATCTTTGGGGCCTGATCAGGAATAGTAGGAACT...\n",
      "12903    AACCTTATATTTCCTATTCGGAGCATGAGCCGGAATATTAGGAACA...\n",
      "12904    ----------------------------------------------...\n",
      "12905    AACATTATATTTTATTTTTGGGGCTTGAGCTGGAATAGTTGGAACT...\n",
      "Name: dna, Length: 12906, dtype: object\n",
      "0          33\n",
      "1         634\n",
      "2        1175\n",
      "3          32\n",
      "4         468\n",
      "         ... \n",
      "12901     400\n",
      "12902     171\n",
      "12903     329\n",
      "12904    1148\n",
      "12905     406\n",
      "Name: labels, Length: 12906, dtype: int64\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(x_train.dna)\n",
    "print(y_train)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsGrjgtnlO5o"
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "65ZxGvl_lO5o"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Converting base pairs, ACTG, into numbers\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(x_train.dna)\n",
    "sequence_of_int_train = tokenizer.texts_to_sequences(x_train.dna)\n",
    "sequence_of_int_test = tokenizer.texts_to_sequences(x_test.dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c1ZptH66lO5p"
   },
   "outputs": [],
   "source": [
    "Ntrain=len(sequence_of_int_train)\n",
    "Ntest=len(sequence_of_int_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4Z_gEJRklO5p"
   },
   "outputs": [],
   "source": [
    "\n",
    "sl = 658\n",
    "Xtrain=np.zeros((Ntrain,sl,5))\n",
    "Xtest=np.zeros((Ntest,sl,5))\n",
    "for i in range(Ntrain):\n",
    "    Nt=len(sequence_of_int_train[i])\n",
    "\n",
    "    for j in range(sl):\n",
    "        if(len(sequence_of_int_train[i])>j):\n",
    "            k=sequence_of_int_train[i][j]-1\n",
    "            if(k>4):\n",
    "                k=4\n",
    "            Xtrain[i][j][k]=1.0\n",
    "            \n",
    "for i in range(Ntest):\n",
    "    Nt=len(sequence_of_int_test[i])\n",
    "\n",
    "    for j in range(sl):\n",
    "        if(len(sequence_of_int_test[i])>j):\n",
    "            k=sequence_of_int_test[i][j]-1\n",
    "            if(k>4):\n",
    "                k=4\n",
    "            Xtest[i][j][k]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "53FSpziFlO5s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  labels\n",
      "0          1      33\n",
      "1          2     634\n",
      "2          3    1175\n",
      "3          4      32\n",
      "4          5     468\n",
      "...      ...     ...\n",
      "12901  12902     400\n",
      "12902  12903     171\n",
      "12903  12904     329\n",
      "12904  12905    1148\n",
      "12905  12906     406\n",
      "\n",
      "[12906 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "M-3eSkb1lO5s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12906, 658, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Expanding the training set shape for CNN \n",
    "Xtrain=np.expand_dims(Xtrain, axis=3)\n",
    "Xtest=np.expand_dims(Xtest, axis=3)\n",
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "G67skRQ7lO5s"
   },
   "outputs": [],
   "source": [
    "# for now just set test -1s to pos int outside of range to check error locally\n",
    "for k in range(len(y_test)):\n",
    "    if y_test[k]==-1:\n",
    "        y_test[k]=1217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iBw-MBoJlO5s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213\n"
     ]
    }
   ],
   "source": [
    "print(np.max(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sNcXPXklO5s"
   },
   "source": [
    "### Classic CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uYxKTPTzlO5s"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0gKLIQSglO5s"
   },
   "outputs": [],
   "source": [
    "# CNN model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu', input_shape=(sl, 5,1),padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(500, activation='tanh'))\n",
    "model.add(layers.Dense(1218))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "s_NK_Z3BlO5t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 658, 5, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 658, 5, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 219, 5, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 219, 5, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 219, 5, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 73, 5, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 73, 5, 16)         4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 73, 5, 16)         64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 24, 5, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 1920)              7680      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               960500    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1218)              610218    \n",
      "=================================================================\n",
      "Total params: 1,602,574\n",
      "Trainable params: 1,598,510\n",
      "Non-trainable params: 4,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "xGmfiS1TlO5t"
   },
   "outputs": [],
   "source": [
    "# Step-decay learning rate scheduler\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.001\n",
    "   drop = 0.5\n",
    "   epochs_drop = 2.0\n",
    "   lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "class LossHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))\n",
    "        \n",
    "loss_history = LossHistory()\n",
    "lrate = callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "wCHEyNB1lO5t",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "404/404 [==============================] - 43s 106ms/step - loss: 1.8177 - accuracy: 0.7784 - top_k_categorical_accuracy: 2.3245e-04 - val_loss: 4.9266 - val_accuracy: 0.1539 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "404/404 [==============================] - 43s 106ms/step - loss: 0.0874 - accuracy: 0.9888 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.3310 - val_accuracy: 0.9558 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "404/404 [==============================] - 44s 108ms/step - loss: 0.0291 - accuracy: 0.9959 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.3015 - val_accuracy: 0.9588 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "404/404 [==============================] - 45s 112ms/step - loss: 0.0170 - accuracy: 0.9970 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.2929 - val_accuracy: 0.9598 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "404/404 [==============================] - 45s 111ms/step - loss: 0.0128 - accuracy: 0.9976 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.2925 - val_accuracy: 0.9583 - val_top_k_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "import tensorflow as tf\n",
    "#opt = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "\n",
    "# Validation time\n",
    "history = model.fit(Xtrain, y_train, epochs=5, batch_size = 32, validation_data=(Xtest, y_test), callbacks=callbacks_list, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "PrGe9jvHlO5t"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_new_train=Xtrain.reshape((Xtrain.shape[0], 658*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "hHJ8QC9KlO5u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "101/101 [==============================] - 2s 14ms/step - loss: 682.4698\n",
      "Epoch 2/30\n",
      "101/101 [==============================] - 1s 15ms/step - loss: 427.0390\n",
      "Epoch 3/30\n",
      "101/101 [==============================] - 1s 14ms/step - loss: 376.2299\n",
      "Epoch 4/30\n",
      "101/101 [==============================] - 1s 14ms/step - loss: 344.0372\n",
      "Epoch 5/30\n",
      "101/101 [==============================] - 1s 14ms/step - loss: 319.8466\n",
      "Epoch 6/30\n",
      "101/101 [==============================] - 1s 14ms/step - loss: 299.6163\n",
      "Epoch 7/30\n",
      "101/101 [==============================] - 1s 14ms/step - loss: 282.7689\n",
      "Epoch 8/30\n",
      "101/101 [==============================] - 1s 15ms/step - loss: 268.0807\n",
      "Epoch 9/30\n",
      "101/101 [==============================] - 1s 15ms/step - loss: 255.2028\n",
      "Epoch 10/30\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 243.4993\n",
      "Epoch 11/30\n",
      "101/101 [==============================] - 1s 15ms/step - loss: 233.1757\n",
      "Epoch 12/30\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 223.8692\n",
      "Epoch 13/30\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 215.6568\n",
      "Epoch 14/30\n",
      "101/101 [==============================] - 1s 15ms/step - loss: 207.8758\n",
      "Epoch 15/30\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 200.5919\n",
      "Epoch 16/30\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 193.7274\n",
      "Epoch 17/30\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 187.7920\n",
      "Epoch 18/30\n",
      "101/101 [==============================] - 1s 15ms/step - loss: 181.9780\n",
      "Epoch 19/30\n",
      "101/101 [==============================] - 2s 16ms/step - loss: 176.7951\n",
      "Epoch 20/30\n",
      "101/101 [==============================] - 2s 16ms/step - loss: 172.0584\n",
      "Epoch 21/30\n",
      "101/101 [==============================] - 2s 17ms/step - loss: 167.5760\n",
      "Epoch 22/30\n",
      "101/101 [==============================] - 2s 16ms/step - loss: 162.9472\n",
      "Epoch 23/30\n",
      "101/101 [==============================] - 2s 15ms/step - loss: 159.1695\n",
      "Epoch 24/30\n",
      "101/101 [==============================] - 2s 17ms/step - loss: 155.3191\n",
      "Epoch 25/30\n",
      "101/101 [==============================] - 2s 16ms/step - loss: 152.0876\n",
      "Epoch 26/30\n",
      "101/101 [==============================] - 2s 17ms/step - loss: 148.4861\n",
      "Epoch 27/30\n",
      "101/101 [==============================] - 2s 17ms/step - loss: 146.1020\n",
      "Epoch 28/30\n",
      "101/101 [==============================] - 2s 16ms/step - loss: 142.7263\n",
      "Epoch 29/30\n",
      "101/101 [==============================] - 2s 17ms/step - loss: 139.9113\n",
      "Epoch 30/30\n",
      "101/101 [==============================] - 2s 17ms/step - loss: 137.9681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f911169d550>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sampling layer\n",
    "original_dim=658*5\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "intermediate_dim = 256\n",
    "\n",
    "# can change to 50 or 2\n",
    "latent_dim = 50\n",
    "epoch_num=30\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "reconstruction_loss = binary_crossentropy(original_inputs,outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "\n",
    "# Train.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(X_new_train, X_new_train, epochs=epoch_num, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "dnWhNkwPlO5u"
   },
   "outputs": [],
   "source": [
    "x_train_encoded = encoder.predict(X_new_train, batch_size=36)\n",
    "X_test_new=Xtest.reshape((Xtest.shape[0], 658*5))\n",
    "\n",
    "x_test_encoded = encoder.predict(X_test_new, batch_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train simple MLP classifier using VAE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "404/404 [==============================] - 2s 4ms/step - loss: 1.8234 - accuracy: 0.7555 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.6804 - val_accuracy: 0.9065 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.2390 - accuracy: 0.9549 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5600 - val_accuracy: 0.9298 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.1573 - accuracy: 0.9662 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.9313 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.1042 - accuracy: 0.9764 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5499 - val_accuracy: 0.9370 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0861 - accuracy: 0.9807 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5581 - val_accuracy: 0.9367 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0693 - accuracy: 0.9844 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5585 - val_accuracy: 0.9388 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0638 - accuracy: 0.9864 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5632 - val_accuracy: 0.9386 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0560 - accuracy: 0.9892 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5644 - val_accuracy: 0.9392 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0538 - accuracy: 0.9898 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5672 - val_accuracy: 0.9394 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0498 - accuracy: 0.9912 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5680 - val_accuracy: 0.9396 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0489 - accuracy: 0.9913 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5691 - val_accuracy: 0.9399 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0469 - accuracy: 0.9926 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.9400 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0464 - accuracy: 0.9925 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5705 - val_accuracy: 0.9399 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0453 - accuracy: 0.9927 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5708 - val_accuracy: 0.9400 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0450 - accuracy: 0.9928 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.9400 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0444 - accuracy: 0.9929 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5714 - val_accuracy: 0.9402 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0443 - accuracy: 0.9929 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.9400 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9931 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.9400 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0439 - accuracy: 0.9931 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.9400 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 0.0438 - accuracy: 0.9933 - top_k_categorical_accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.9400 - val_top_k_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(300))\n",
    "model2.add(layers.Dense(1218))\n",
    "\n",
    "model2.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "\n",
    "history = model2.fit(x_train_encoded, y_train, epochs=20, batch_size = 32, validation_data=(x_test_encoded, y_test), callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DNA_embedddings_INSECT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
